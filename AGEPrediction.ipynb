{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec1d4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.layers import Conv2D,MaxPool2D,Dense,Dropout,BatchNormalization,Flatten,Input\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0cae6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"UTKFace/UTKFace\"\n",
    "data = []\n",
    "age = []\n",
    "for img in os.listdir(path):\n",
    "  ages = img.split(\"_\")[0]\n",
    "  img = cv2.imread(str(path)+\"/\"+str(img))\n",
    "  img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "  data.append(np.array(img))\n",
    "  age.append(np.array(ages))\n",
    "age = np.array(age,dtype=np.int64)\n",
    "data = np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd339225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23708, 200, 200, 3)\n",
      "(23708,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(age.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "28ba8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "x_train,x_test,y_train,y_test = train_test_split(data,age,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "66b1a803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_26 (InputLayer)        [(None, 200, 200, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 199, 199, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling (None, 99, 99, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_60 (Batc (None, 99, 99, 16)        64        \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 99, 99, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 98, 98, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_61 (Batc (None, 49, 49, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 49, 49, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_73 (Conv2D)           (None, 48, 48, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_62 (Batc (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 64)                2359360   \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,372,977\n",
      "Trainable params: 2,372,753\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = Input(shape=(200,200,3))\n",
    "conv1 = Conv2D(16,(2,2),activation=\"relu\")(input)\n",
    "pool1 = MaxPool2D((2,2))(conv1)\n",
    "batch1 = BatchNormalization()(pool1)\n",
    "Dropout1 = Dropout(0.5)(batch1)\n",
    "\n",
    "conv2 = Conv2D(32,(2,2),activation=\"relu\")(Dropout1)\n",
    "pool2 = MaxPool2D((2,2))(conv2)\n",
    "batch2 = BatchNormalization()(pool2)\n",
    "Dropout2 = Dropout(0.5)(batch2)\n",
    "\n",
    "\n",
    "conv3 = Conv2D(64,(2,2),activation=\"relu\")(Dropout2)\n",
    "pool3 = MaxPool2D((2,2))(conv3)\n",
    "batch3 = BatchNormalization()(pool3)\n",
    "Dropout3 = Dropout(0.5)(batch3)\n",
    "\n",
    "flt = Flatten()(batch3)\n",
    "age_l = Dense(64,activation=\"relu\")(flt)\n",
    "age_l = Dense(32,activation=\"relu\")(age_l)\n",
    "age_l = Dense(16,activation=\"relu\")(age_l)\n",
    "age_l = Dense(1,activation=\"sigmoid\")(age_l)\n",
    "\n",
    "model = Model(inputs=input,outputs= age_l)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0c18de26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "126/126 [==============================] - 196s 2s/step - loss: 1421.7493 - mae: 32.0776 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 2/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1452.5786 - mae: 32.4330 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 3/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1437.0786 - mae: 32.1102 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 4/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1418.9166 - mae: 32.1555 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 5/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1417.2246 - mae: 32.1985 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 6/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1419.7277 - mae: 32.1628 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 7/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1443.1011 - mae: 32.3939 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 8/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1434.5634 - mae: 32.2429 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 9/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1434.6738 - mae: 32.2444 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 10/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1430.2099 - mae: 32.1995 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 11/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1443.7269 - mae: 32.3831 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 12/100\n",
      "126/126 [==============================] - 195s 2s/step - loss: 1436.1196 - mae: 32.3349 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 13/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1441.1387 - mae: 32.3509 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 14/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1440.3280 - mae: 32.3710 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 15/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1439.7441 - mae: 32.2741 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 16/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1449.9190 - mae: 32.3981 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 17/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1433.7411 - mae: 32.2241 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 18/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1432.4464 - mae: 32.2535 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 19/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1448.3856 - mae: 32.4596 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 20/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1424.1538 - mae: 32.2104 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 21/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1412.4125 - mae: 31.8856 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 22/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1445.7822 - mae: 32.4565 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 23/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1410.4590 - mae: 31.9907 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 24/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1425.8101 - mae: 32.2392 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 25/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1432.3077 - mae: 32.2373 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 26/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1428.7168 - mae: 32.1561 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 27/100\n",
      "126/126 [==============================] - 197s 2s/step - loss: 1427.1883 - mae: 32.1068 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 28/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1404.6509 - mae: 31.8306 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 29/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1425.6313 - mae: 32.1213 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 30/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1446.6537 - mae: 32.3744 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 31/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1411.0482 - mae: 31.9559 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 32/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1450.2469 - mae: 32.5061 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 33/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1434.0915 - mae: 32.2932 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 34/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1446.3603 - mae: 32.4429 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 35/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1430.4157 - mae: 32.2153 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 36/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1404.7176 - mae: 31.9186 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 37/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1465.5009 - mae: 32.6498 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 38/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1442.4919 - mae: 32.3943 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 39/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1440.4920 - mae: 32.3034 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 40/100\n",
      "126/126 [==============================] - 191s 2s/step - loss: 1413.5492 - mae: 31.9523 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 41/100\n",
      "126/126 [==============================] - 191s 2s/step - loss: 1428.4638 - mae: 32.2188 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 42/100\n",
      "126/126 [==============================] - 191s 2s/step - loss: 1415.4601 - mae: 32.0734 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 43/100\n",
      "126/126 [==============================] - 191s 2s/step - loss: 1448.7874 - mae: 32.4739 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 44/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1419.7459 - mae: 32.0580 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 45/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1453.3490 - mae: 32.4482 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 46/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1439.9821 - mae: 32.2482 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 47/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1432.4734 - mae: 32.2968 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 48/100\n",
      "126/126 [==============================] - 197s 2s/step - loss: 1444.6408 - mae: 32.4137 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 49/100\n",
      "126/126 [==============================] - 196s 2s/step - loss: 1446.5450 - mae: 32.3633 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 50/100\n",
      "126/126 [==============================] - 196s 2s/step - loss: 1440.9014 - mae: 32.4022 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 51/100\n",
      "126/126 [==============================] - 195s 2s/step - loss: 1430.8113 - mae: 32.3198 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 52/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1443.2245 - mae: 32.3268 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 53/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1441.5660 - mae: 32.3519 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 54/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1437.2112 - mae: 32.3770 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 55/100\n",
      "126/126 [==============================] - 196s 2s/step - loss: 1438.5367 - mae: 32.2705 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 56/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1449.4193 - mae: 32.4704 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 57/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1461.7634 - mae: 32.5799 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 58/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1428.6836 - mae: 32.2226 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 59/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1439.3208 - mae: 32.3439 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 60/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1446.9702 - mae: 32.4217 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 61/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1420.5437 - mae: 32.1204 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 62/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1415.9796 - mae: 32.0781 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 63/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1421.7148 - mae: 32.1497 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 64/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1442.8630 - mae: 32.4209 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 65/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1417.0842 - mae: 32.0871 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 66/100\n",
      "126/126 [==============================] - 194s 2s/step - loss: 1411.5939 - mae: 32.0862 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 67/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1449.3536 - mae: 32.4916 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 68/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1419.8434 - mae: 32.0979 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 69/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1437.6355 - mae: 32.3198 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 70/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1423.4108 - mae: 32.1724 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 71/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1438.1838 - mae: 32.2900 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 72/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1461.7929 - mae: 32.6469 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 73/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1440.8376 - mae: 32.3073 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 74/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1446.8514 - mae: 32.3982 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 75/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1449.2421 - mae: 32.4391 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 76/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1430.5865 - mae: 32.1821 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 77/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1460.7867 - mae: 32.5858 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 78/100\n",
      "126/126 [==============================] - 193s 2s/step - loss: 1439.5647 - mae: 32.2706 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 79/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1439.7770 - mae: 32.3688 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 80/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1435.3616 - mae: 32.2440 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 81/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1429.7100 - mae: 32.1981 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 82/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1414.5030 - mae: 32.0892 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 83/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1427.1608 - mae: 32.2719 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 84/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1420.1058 - mae: 32.0763 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 85/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1420.2751 - mae: 32.1283 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 86/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1451.0445 - mae: 32.4139 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 87/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1427.7226 - mae: 32.1670 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 88/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1420.9804 - mae: 32.1382 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 89/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1439.7169 - mae: 32.2680 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 90/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1455.2560 - mae: 32.5030 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 91/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1443.1789 - mae: 32.3736 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 92/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1432.8839 - mae: 32.2517 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 93/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1421.9339 - mae: 32.0923 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 94/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1438.5199 - mae: 32.3465 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 95/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1416.2688 - mae: 32.1054 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 96/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1419.3581 - mae: 32.0818 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 97/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1435.5058 - mae: 32.2532 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 98/100\n",
      "126/126 [==============================] - 192s 2s/step - loss: 1415.4933 - mae: 32.0375 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 99/100\n",
      "126/126 [==============================] - 191s 2s/step - loss: 1438.8641 - mae: 32.2757 - val_loss: 1470.6379 - val_mae: 32.5447\n",
      "Epoch 100/100\n",
      "126/126 [==============================] - 191s 2s/step - loss: 1442.0549 - mae: 32.2280 - val_loss: 1470.6379 - val_mae: 32.5447\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"mse\",metrics=['mae'])\n",
    "model_CNN  = model.fit(x_train, y_train,\n",
    "                        validation_split = 0.1,\n",
    "                        batch_size = 128,\n",
    "                        epochs=100)\n",
    "model.save(\"Agemodel2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b815c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186/186 [==============================] - 23s 124ms/step - loss: 1447.1897 - mae: 32.4102\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "test_model = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2453a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelAgeregression2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e3ea539f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.41015691686405"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "predicted = model.predict(x_test)\n",
    "mse=mean_absolute_error(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb424c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

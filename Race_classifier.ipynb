{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd8c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "dataset_folder_name = 'UTKFace/UTKFace'\n",
    "TRAIN_TEST_SPLIT = 0.9\n",
    "IM_WIDTH = IM_HEIGHT = 100\n",
    "dataset_dict = {\n",
    "    'race_id': {\n",
    "        0: 'white', \n",
    "        1: 'black', \n",
    "        2: 'asian', \n",
    "        3: 'indian', \n",
    "        4: 'others'\n",
    "    },\n",
    "    'gender_id': {\n",
    "        0: 'male',\n",
    "        1: 'female'\n",
    "    }\n",
    "}\n",
    "dataset_dict['gender_alias'] = dict((g, i) for i, g in dataset_dict['gender_id'].items())\n",
    "dataset_dict['race_alias'] = dict((r, i) for i, r in dataset_dict['race_id'].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d078aadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>UTKFace/UTKFace\\100_0_0_20170112213500903.jpg....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "      <td>UTKFace/UTKFace\\100_0_0_20170112215240346.jpg....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>UTKFace/UTKFace\\100_1_0_20170110183726390.jpg....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>UTKFace/UTKFace\\100_1_0_20170112213001988.jpg....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "      <td>UTKFace/UTKFace\\100_1_0_20170112213303693.jpg....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  gender   race                                               file\n",
       "0  100.0    male  white  UTKFace/UTKFace\\100_0_0_20170112213500903.jpg....\n",
       "1  100.0    male  white  UTKFace/UTKFace\\100_0_0_20170112215240346.jpg....\n",
       "2  100.0  female  white  UTKFace/UTKFace\\100_1_0_20170110183726390.jpg....\n",
       "3  100.0  female  white  UTKFace/UTKFace\\100_1_0_20170112213001988.jpg....\n",
       "4  100.0  female  white  UTKFace/UTKFace\\100_1_0_20170112213303693.jpg...."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_dataset(dataset_path, ext='jpg'):\n",
    "    \"\"\"\n",
    "    Used to extract information about our dataset. It does iterate over all images and return a DataFrame with\n",
    "    the data (age, gender and sex) of all files.\n",
    "    \"\"\"\n",
    "    def parse_info_from_file(path):\n",
    "        \"\"\"\n",
    "        Parse information from a single file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filename = os.path.split(path)[1]\n",
    "            filename = os.path.splitext(filename)[0]\n",
    "            age, gender, race, _ = filename.split('_')\n",
    "            return int(age), dataset_dict['gender_id'][int(gender)], dataset_dict['race_id'][int(race)]\n",
    "        except Exception as ex:\n",
    "            return None, None, None\n",
    "        \n",
    "    files = glob.glob(os.path.join(dataset_path, \"*.%s\" % ext))\n",
    "    \n",
    "    records = []\n",
    "    for file in files:\n",
    "        info = parse_info_from_file(file)\n",
    "        records.append(info)\n",
    "        \n",
    "    df = pd.DataFrame(records)\n",
    "    df['file'] = files\n",
    "    df.columns = ['age', 'gender', 'race', 'file']\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "df = parse_dataset(dataset_folder_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6aee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from PIL import Image\n",
    "class UtkFaceDataGenerator():\n",
    "    \"\"\"\n",
    "    Data generator for the UTKFace dataset. This class should be used when training our Keras multi-output model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        \n",
    "    def generate_split_indexes(self):\n",
    "        p = np.random.permutation(len(self.df))\n",
    "        train_up_to = int(len(self.df) * TRAIN_TEST_SPLIT)\n",
    "        train_idx = p[:train_up_to]\n",
    "        test_idx = p[train_up_to:]\n",
    "        train_up_to = int(train_up_to * TRAIN_TEST_SPLIT)\n",
    "        train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]\n",
    "        \n",
    "        # converts alias to id\n",
    "        self.df['gender_id'] = self.df['gender'].map(lambda gender: dataset_dict['gender_alias'][gender])\n",
    "        self.df['race_id'] = self.df['race'].map(lambda race: dataset_dict['race_alias'][race])\n",
    "        self.max_age = self.df['age'].max()\n",
    "        \n",
    "        return train_idx, valid_idx, test_idx\n",
    "    \n",
    "    def preprocess_image(self, img_path):\n",
    "        \"\"\"\n",
    "        Used to perform some minor preprocessing on the image before inputting into the network.\n",
    "        \"\"\"\n",
    "        im = Image.open(img_path)\n",
    "        im = im.resize((IM_WIDTH, IM_HEIGHT))\n",
    "        im = np.array(im) / 255.0\n",
    "        \n",
    "        return im\n",
    "        \n",
    "    def generate_images(self, image_idx, is_training, batch_size=16):\n",
    "        \"\"\"\n",
    "        Used to generate a batch with images when training/testing/validating our Keras model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # arrays to store our batched data\n",
    "        images, ages, races, genders = [], [], [], []\n",
    "        while True:\n",
    "            for idx in image_idx:\n",
    "                person = self.df.iloc[idx]\n",
    "                \n",
    "                age = person['age']\n",
    "                race = person['race_id']\n",
    "                gender = person['gender_id']\n",
    "                file = person['file']\n",
    "                \n",
    "                im = self.preprocess_image(file)\n",
    "                \n",
    "                ages.append(age / self.max_age)\n",
    "                races.append(to_categorical(race, len(dataset_dict['race_id'])))\n",
    "                genders.append(to_categorical(gender, len(dataset_dict['gender_id'])))\n",
    "                images.append(im)\n",
    "                \n",
    "                # yielding condition\n",
    "                if len(images) >= batch_size:\n",
    "                    yield np.array(images), [np.array(ages), np.array(races), np.array(genders)]\n",
    "                    images, ages, races, genders = [], [], [], []\n",
    "                    \n",
    "            if not is_training:\n",
    "                break\n",
    "                \n",
    "data_generator = UtkFaceDataGenerator(df)\n",
    "train_idx, valid_idx, test_idx = data_generator.generate_split_indexes() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189473bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf\n",
    "class UtkMultiOutputModel():\n",
    "    \"\"\"\n",
    "    Used to generate our multi-output model. This CNN contains three branches, one for age, other for \n",
    "    sex and another for race. Each branch contains a sequence of Convolutional Layers that is defined\n",
    "    on the make_default_hidden_layers method.\n",
    "    \"\"\"\n",
    "    def make_default_hidden_layers(self, inputs):\n",
    "        \"\"\"\n",
    "        Used to generate a default set of hidden layers. The structure used in this network is defined as:\n",
    "        \n",
    "        Conv2D -> BatchNormalization -> Pooling -> Dropout\n",
    "        \"\"\"\n",
    "        x = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        x = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization(axis=-1)(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "        return x\n",
    "    def build_race_branch(self, inputs, num_races):\n",
    "        \"\"\"\n",
    "        Used to build the race branch of our face recognition network.\n",
    "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
    "        followed by the Dense output layer.\n",
    "        \"\"\"\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(64)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_races)(x)\n",
    "        x = Activation(\"softmax\", name=\"race_output\")(x)\n",
    "        return x\n",
    "    def build_gender_branch(self, inputs, num_genders=2):\n",
    "        \"\"\"\n",
    "        Used to build the gender branch of our face recognition network.\n",
    "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
    "        followed by the Dense output layer.\n",
    "        \"\"\"\n",
    "        x = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(2)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(num_genders)(x)\n",
    "        x = Activation(\"sigmoid\", name=\"gender_output\")(x)\n",
    "        return x\n",
    "    def build_age_branch(self, inputs):   \n",
    "        \"\"\"\n",
    "        Used to build the age branch of our face recognition network.\n",
    "        This branch is composed of three Conv -> BN -> Pool -> Dropout blocks, \n",
    "        followed by the Dense output layer.\n",
    "        \"\"\"\n",
    "        x = self.make_default_hidden_layers(inputs)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(32)(x)\n",
    "        x = Activation(\"relu\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation(\"linear\", name=\"age_output\")(x)\n",
    "        return x\n",
    "    def assemble_full_model(self, width, height, num_races):\n",
    "        \"\"\"\n",
    "        Used to assemble our multi-output model CNN.\n",
    "        \"\"\"\n",
    "        input_shape = (height, width, 3)\n",
    "        inputs = Input(shape=input_shape)\n",
    "        age_branch = self.build_age_branch(inputs)\n",
    "        race_branch = self.build_race_branch(inputs, num_races)\n",
    "        gender_branch = self.build_gender_branch(inputs)\n",
    "        model = Model(inputs=inputs,\n",
    "                     outputs = [age_branch, race_branch, gender_branch],\n",
    "                     name=\"face_net\")\n",
    "        return model\n",
    "    \n",
    "model = UtkMultiOutputModel().assemble_full_model(IM_WIDTH, IM_HEIGHT, num_races=len(dataset_dict['race_alias']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "852d1118",
   "metadata": {},
   "outputs": [],
   "source": [
    "foptimizer_function = 'adam'\n",
    "model.compile(optimizer=foptimizer_function, \n",
    "              loss={\n",
    "                  'age_output': 'mse', \n",
    "                  'race_output': 'categorical_crossentropy', \n",
    "                  'gender_output': 'binary_crossentropy'},\n",
    "              loss_weights={\n",
    "                  'age_output': 4., \n",
    "                  'race_output': 1.5, \n",
    "                  'gender_output': 0.1},\n",
    "              metrics={\n",
    "                  'age_output': 'mae', \n",
    "                  'race_output': 'accuracy',\n",
    "                  'gender_output': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6b92d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nhung\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1915: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "600/600 [==============================] - 261s 388ms/step - loss: 7.0541 - age_output_loss: 1.1539 - race_output_loss: 1.5822 - gender_output_loss: 0.6503 - age_output_mae: 0.7477 - race_output_accuracy: 0.4604 - gender_output_accuracy: 0.6074 - val_loss: 4.4407 - val_age_output_loss: 0.7210 - val_race_output_loss: 0.9962 - val_gender_output_loss: 0.6239 - val_age_output_mae: 0.3331 - val_race_output_accuracy: 0.6700 - val_gender_output_accuracy: 0.6989\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 2/64\n",
      "600/600 [==============================] - 230s 384ms/step - loss: 1.7203 - age_output_loss: 0.0611 - race_output_loss: 0.9474 - gender_output_loss: 0.5484 - age_output_mae: 0.1885 - race_output_accuracy: 0.6545 - gender_output_accuracy: 0.6796 - val_loss: 5.0173 - val_age_output_loss: 0.9477 - val_race_output_loss: 0.7900 - val_gender_output_loss: 0.4128 - val_age_output_mae: 0.3280 - val_race_output_accuracy: 0.7225 - val_gender_output_accuracy: 0.8546\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 3/64\n",
      "600/600 [==============================] - 235s 391ms/step - loss: 1.4698 - age_output_loss: 0.0260 - race_output_loss: 0.8747 - gender_output_loss: 0.5367 - age_output_mae: 0.1243 - race_output_accuracy: 0.6893 - gender_output_accuracy: 0.6806 - val_loss: 3.6061 - val_age_output_loss: 0.6229 - val_race_output_loss: 0.7149 - val_gender_output_loss: 0.4214 - val_age_output_mae: 0.2249 - val_race_output_accuracy: 0.7476 - val_gender_output_accuracy: 0.8144\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 4/64\n",
      "600/600 [==============================] - 236s 393ms/step - loss: 1.3791 - age_output_loss: 0.0210 - race_output_loss: 0.8298 - gender_output_loss: 0.5028 - age_output_mae: 0.1109 - race_output_accuracy: 0.7019 - gender_output_accuracy: 0.7324 - val_loss: 1.7003 - val_age_output_loss: 0.1558 - val_race_output_loss: 0.6946 - val_gender_output_loss: 0.3500 - val_age_output_mae: 0.1429 - val_race_output_accuracy: 0.7670 - val_gender_output_accuracy: 0.8636\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 5/64\n",
      "600/600 [==============================] - 235s 392ms/step - loss: 1.3182 - age_output_loss: 0.0189 - race_output_loss: 0.7980 - gender_output_loss: 0.4542 - age_output_mae: 0.1056 - race_output_accuracy: 0.7197 - gender_output_accuracy: 0.7578 - val_loss: 1.3891 - val_age_output_loss: 0.0724 - val_race_output_loss: 0.7081 - val_gender_output_loss: 0.3731 - val_age_output_mae: 0.1315 - val_race_output_accuracy: 0.7514 - val_gender_output_accuracy: 0.8480\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 6/64\n",
      "600/600 [==============================] - 234s 390ms/step - loss: 1.2834 - age_output_loss: 0.0176 - race_output_loss: 0.7797 - gender_output_loss: 0.4349 - age_output_mae: 0.1016 - race_output_accuracy: 0.7276 - gender_output_accuracy: 0.7778 - val_loss: 1.5025 - val_age_output_loss: 0.1084 - val_race_output_loss: 0.6887 - val_gender_output_loss: 0.3586 - val_age_output_mae: 0.1295 - val_race_output_accuracy: 0.7642 - val_gender_output_accuracy: 0.8575\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 7/64\n",
      "600/600 [==============================] - 239s 399ms/step - loss: 1.2407 - age_output_loss: 0.0168 - race_output_loss: 0.7534 - gender_output_loss: 0.4332 - age_output_mae: 0.0994 - race_output_accuracy: 0.7372 - gender_output_accuracy: 0.7785 - val_loss: 1.3754 - val_age_output_loss: 0.0674 - val_race_output_loss: 0.7160 - val_gender_output_loss: 0.3165 - val_age_output_mae: 0.1433 - val_race_output_accuracy: 0.7528 - val_gender_output_accuracy: 0.8589\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 8/64\n",
      "600/600 [==============================] - 242s 403ms/step - loss: 1.2232 - age_output_loss: 0.0158 - race_output_loss: 0.7452 - gender_output_loss: 0.4236 - age_output_mae: 0.0966 - race_output_accuracy: 0.7400 - gender_output_accuracy: 0.7781 - val_loss: 1.1094 - val_age_output_loss: 0.0236 - val_race_output_loss: 0.6560 - val_gender_output_loss: 0.3090 - val_age_output_mae: 0.1333 - val_race_output_accuracy: 0.7737 - val_gender_output_accuracy: 0.8736\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 9/64\n",
      "600/600 [==============================] - 245s 408ms/step - loss: 1.2004 - age_output_loss: 0.0150 - race_output_loss: 0.7321 - gender_output_loss: 0.4205 - age_output_mae: 0.0939 - race_output_accuracy: 0.7456 - gender_output_accuracy: 0.7856 - val_loss: 1.5216 - val_age_output_loss: 0.1104 - val_race_output_loss: 0.7000 - val_gender_output_loss: 0.2980 - val_age_output_mae: 0.1489 - val_race_output_accuracy: 0.7656 - val_gender_output_accuracy: 0.8812\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 10/64\n",
      "600/600 [==============================] - 247s 412ms/step - loss: 1.1730 - age_output_loss: 0.0146 - race_output_loss: 0.7151 - gender_output_loss: 0.4202 - age_output_mae: 0.0923 - race_output_accuracy: 0.7477 - gender_output_accuracy: 0.7758 - val_loss: 1.1047 - val_age_output_loss: 0.0149 - val_race_output_loss: 0.6772 - val_gender_output_loss: 0.2938 - val_age_output_mae: 0.0863 - val_race_output_accuracy: 0.7675 - val_gender_output_accuracy: 0.8707\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 11/64\n",
      "600/600 [==============================] - 232s 386ms/step - loss: 1.1425 - age_output_loss: 0.0139 - race_output_loss: 0.6977 - gender_output_loss: 0.4045 - age_output_mae: 0.0901 - race_output_accuracy: 0.7567 - gender_output_accuracy: 0.7881 - val_loss: 1.2218 - val_age_output_loss: 0.0472 - val_race_output_loss: 0.6693 - val_gender_output_loss: 0.2916 - val_age_output_mae: 0.1846 - val_race_output_accuracy: 0.7812 - val_gender_output_accuracy: 0.8764\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 12/64\n",
      "600/600 [==============================] - 246s 410ms/step - loss: 1.1519 - age_output_loss: 0.0137 - race_output_loss: 0.7046 - gender_output_loss: 0.4016 - age_output_mae: 0.0895 - race_output_accuracy: 0.7558 - gender_output_accuracy: 0.7856 - val_loss: 1.1691 - val_age_output_loss: 0.0289 - val_race_output_loss: 0.6834 - val_gender_output_loss: 0.2841 - val_age_output_mae: 0.1482 - val_race_output_accuracy: 0.7694 - val_gender_output_accuracy: 0.8864\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 13/64\n",
      "600/600 [==============================] - 248s 414ms/step - loss: 1.1315 - age_output_loss: 0.0132 - race_output_loss: 0.6924 - gender_output_loss: 0.4013 - age_output_mae: 0.0876 - race_output_accuracy: 0.7577 - gender_output_accuracy: 0.7852 - val_loss: 1.0684 - val_age_output_loss: 0.0237 - val_race_output_loss: 0.6297 - val_gender_output_loss: 0.2894 - val_age_output_mae: 0.1339 - val_race_output_accuracy: 0.7803 - val_gender_output_accuracy: 0.8802\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 14/64\n",
      "600/600 [==============================] - 249s 416ms/step - loss: 1.1098 - age_output_loss: 0.0124 - race_output_loss: 0.6806 - gender_output_loss: 0.3947 - age_output_mae: 0.0850 - race_output_accuracy: 0.7608 - gender_output_accuracy: 0.7987 - val_loss: 1.2846 - val_age_output_loss: 0.0730 - val_race_output_loss: 0.6426 - val_gender_output_loss: 0.2858 - val_age_output_mae: 0.2247 - val_race_output_accuracy: 0.7760 - val_gender_output_accuracy: 0.8845\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 15/64\n",
      "600/600 [==============================] - 250s 416ms/step - loss: 1.0910 - age_output_loss: 0.0123 - race_output_loss: 0.6685 - gender_output_loss: 0.3925 - age_output_mae: 0.0842 - race_output_accuracy: 0.7664 - gender_output_accuracy: 0.7920 - val_loss: 1.0796 - val_age_output_loss: 0.0342 - val_race_output_loss: 0.6064 - val_gender_output_loss: 0.3333 - val_age_output_mae: 0.1610 - val_race_output_accuracy: 0.7907 - val_gender_output_accuracy: 0.8513\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 16/64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 247s 412ms/step - loss: 1.0772 - age_output_loss: 0.0118 - race_output_loss: 0.6603 - gender_output_loss: 0.3959 - age_output_mae: 0.0824 - race_output_accuracy: 0.7663 - gender_output_accuracy: 0.7928 - val_loss: 0.9644 - val_age_output_loss: 0.0112 - val_race_output_loss: 0.5949 - val_gender_output_loss: 0.2730 - val_age_output_mae: 0.0841 - val_race_output_accuracy: 0.7940 - val_gender_output_accuracy: 0.8878\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 17/64\n",
      "600/600 [==============================] - 243s 406ms/step - loss: 1.0571 - age_output_loss: 0.0116 - race_output_loss: 0.6477 - gender_output_loss: 0.3893 - age_output_mae: 0.0817 - race_output_accuracy: 0.7760 - gender_output_accuracy: 0.7932 - val_loss: 1.1255 - val_age_output_loss: 0.0577 - val_race_output_loss: 0.5792 - val_gender_output_loss: 0.2585 - val_age_output_mae: 0.1296 - val_race_output_accuracy: 0.7959 - val_gender_output_accuracy: 0.8954\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 18/64\n",
      "600/600 [==============================] - 255s 424ms/step - loss: 1.0574 - age_output_loss: 0.0112 - race_output_loss: 0.6489 - gender_output_loss: 0.3936 - age_output_mae: 0.0802 - race_output_accuracy: 0.7706 - gender_output_accuracy: 0.7900 - val_loss: 2.1622 - val_age_output_loss: 0.2392 - val_race_output_loss: 0.7865 - val_gender_output_loss: 0.2559 - val_age_output_mae: 0.2068 - val_race_output_accuracy: 0.7249 - val_gender_output_accuracy: 0.8854\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 19/64\n",
      "600/600 [==============================] - 237s 395ms/step - loss: 1.0390 - age_output_loss: 0.0109 - race_output_loss: 0.6377 - gender_output_loss: 0.3892 - age_output_mae: 0.0794 - race_output_accuracy: 0.7774 - gender_output_accuracy: 0.7988 - val_loss: 1.1569 - val_age_output_loss: 0.0488 - val_race_output_loss: 0.6222 - val_gender_output_loss: 0.2824 - val_age_output_mae: 0.1658 - val_race_output_accuracy: 0.7770 - val_gender_output_accuracy: 0.8868\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 20/64\n",
      "600/600 [==============================] - 250s 418ms/step - loss: 1.0387 - age_output_loss: 0.0105 - race_output_loss: 0.6386 - gender_output_loss: 0.3873 - age_output_mae: 0.0779 - race_output_accuracy: 0.7749 - gender_output_accuracy: 0.7932 - val_loss: 1.0978 - val_age_output_loss: 0.0374 - val_race_output_loss: 0.6146 - val_gender_output_loss: 0.2644 - val_age_output_mae: 0.1068 - val_race_output_accuracy: 0.7865 - val_gender_output_accuracy: 0.8911\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 21/64\n",
      "600/600 [==============================] - 255s 424ms/step - loss: 1.0391 - age_output_loss: 0.0185 - race_output_loss: 0.6178 - gender_output_loss: 0.3845 - age_output_mae: 0.0946 - race_output_accuracy: 0.7857 - gender_output_accuracy: 0.8010 - val_loss: 1.2980 - val_age_output_loss: 0.0554 - val_race_output_loss: 0.7003 - val_gender_output_loss: 0.2608 - val_age_output_mae: 0.1914 - val_race_output_accuracy: 0.7590 - val_gender_output_accuracy: 0.8864\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 22/64\n",
      "600/600 [==============================] - 261s 434ms/step - loss: 1.0231 - age_output_loss: 0.0121 - race_output_loss: 0.6247 - gender_output_loss: 0.3783 - age_output_mae: 0.0834 - race_output_accuracy: 0.7846 - gender_output_accuracy: 0.7997 - val_loss: 1.1771 - val_age_output_loss: 0.0291 - val_race_output_loss: 0.6896 - val_gender_output_loss: 0.2637 - val_age_output_mae: 0.1451 - val_race_output_accuracy: 0.7604 - val_gender_output_accuracy: 0.8883\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 23/64\n",
      "600/600 [==============================] - 264s 441ms/step - loss: 1.0126 - age_output_loss: 0.0114 - race_output_loss: 0.6191 - gender_output_loss: 0.3851 - age_output_mae: 0.0809 - race_output_accuracy: 0.7824 - gender_output_accuracy: 0.7982 - val_loss: 1.0269 - val_age_output_loss: 0.0279 - val_race_output_loss: 0.5928 - val_gender_output_loss: 0.2615 - val_age_output_mae: 0.1393 - val_race_output_accuracy: 0.7973 - val_gender_output_accuracy: 0.8892\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 24/64\n",
      "600/600 [==============================] - 240s 401ms/step - loss: 0.9924 - age_output_loss: 0.0109 - race_output_loss: 0.6067 - gender_output_loss: 0.3900 - age_output_mae: 0.0787 - race_output_accuracy: 0.7884 - gender_output_accuracy: 0.7922 - val_loss: 1.0190 - val_age_output_loss: 0.0280 - val_race_output_loss: 0.5876 - val_gender_output_loss: 0.2548 - val_age_output_mae: 0.1424 - val_race_output_accuracy: 0.7959 - val_gender_output_accuracy: 0.8925\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 25/64\n",
      "600/600 [==============================] - 255s 425ms/step - loss: 0.9815 - age_output_loss: 0.0107 - race_output_loss: 0.5999 - gender_output_loss: 0.3873 - age_output_mae: 0.0782 - race_output_accuracy: 0.7930 - gender_output_accuracy: 0.7969 - val_loss: 1.0046 - val_age_output_loss: 0.0136 - val_race_output_loss: 0.6170 - val_gender_output_loss: 0.2478 - val_age_output_mae: 0.0849 - val_race_output_accuracy: 0.7846 - val_gender_output_accuracy: 0.8977\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 26/64\n",
      "600/600 [==============================] - 250s 416ms/step - loss: 0.9780 - age_output_loss: 0.0105 - race_output_loss: 0.5989 - gender_output_loss: 0.3780 - age_output_mae: 0.0775 - race_output_accuracy: 0.7907 - gender_output_accuracy: 0.8019 - val_loss: 1.0782 - val_age_output_loss: 0.0077 - val_race_output_loss: 0.6802 - val_gender_output_loss: 0.2713 - val_age_output_mae: 0.0657 - val_race_output_accuracy: 0.7666 - val_gender_output_accuracy: 0.8835\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 27/64\n",
      "600/600 [==============================] - 251s 418ms/step - loss: 0.9722 - age_output_loss: 0.0104 - race_output_loss: 0.5954 - gender_output_loss: 0.3768 - age_output_mae: 0.0774 - race_output_accuracy: 0.7870 - gender_output_accuracy: 0.7980 - val_loss: 0.9456 - val_age_output_loss: 0.0075 - val_race_output_loss: 0.5939 - val_gender_output_loss: 0.2484 - val_age_output_mae: 0.0650 - val_race_output_accuracy: 0.7950 - val_gender_output_accuracy: 0.8973\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 28/64\n",
      "600/600 [==============================] - 250s 417ms/step - loss: 0.9579 - age_output_loss: 0.0102 - race_output_loss: 0.5866 - gender_output_loss: 0.3741 - age_output_mae: 0.0762 - race_output_accuracy: 0.7934 - gender_output_accuracy: 0.8044 - val_loss: 1.0202 - val_age_output_loss: 0.0233 - val_race_output_loss: 0.5988 - val_gender_output_loss: 0.2899 - val_age_output_mae: 0.1230 - val_race_output_accuracy: 0.7983 - val_gender_output_accuracy: 0.8783\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 29/64\n",
      "600/600 [==============================] - 257s 429ms/step - loss: 0.9448 - age_output_loss: 0.0100 - race_output_loss: 0.5784 - gender_output_loss: 0.3717 - age_output_mae: 0.0759 - race_output_accuracy: 0.7925 - gender_output_accuracy: 0.8029 - val_loss: 0.9536 - val_age_output_loss: 0.0123 - val_race_output_loss: 0.5851 - val_gender_output_loss: 0.2661 - val_age_output_mae: 0.0900 - val_race_output_accuracy: 0.7978 - val_gender_output_accuracy: 0.8859\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 30/64\n",
      "600/600 [==============================] - 262s 437ms/step - loss: 0.9557 - age_output_loss: 0.0101 - race_output_loss: 0.5851 - gender_output_loss: 0.3758 - age_output_mae: 0.0762 - race_output_accuracy: 0.7952 - gender_output_accuracy: 0.8019 - val_loss: 0.9375 - val_age_output_loss: 0.0073 - val_race_output_loss: 0.5885 - val_gender_output_loss: 0.2577 - val_age_output_mae: 0.0653 - val_race_output_accuracy: 0.7964 - val_gender_output_accuracy: 0.8935\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 31/64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 240s 399ms/step - loss: 0.9442 - age_output_loss: 0.0097 - race_output_loss: 0.5792 - gender_output_loss: 0.3657 - age_output_mae: 0.0749 - race_output_accuracy: 0.7983 - gender_output_accuracy: 0.8033 - val_loss: 1.0773 - val_age_output_loss: 0.0388 - val_race_output_loss: 0.5977 - val_gender_output_loss: 0.2572 - val_age_output_mae: 0.1699 - val_race_output_accuracy: 0.7959 - val_gender_output_accuracy: 0.8906\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 32/64\n",
      "600/600 [==============================] - 254s 423ms/step - loss: 0.9440 - age_output_loss: 0.0098 - race_output_loss: 0.5786 - gender_output_loss: 0.3703 - age_output_mae: 0.0750 - race_output_accuracy: 0.7978 - gender_output_accuracy: 0.8012 - val_loss: 1.0286 - val_age_output_loss: 0.0331 - val_race_output_loss: 0.5787 - val_gender_output_loss: 0.2807 - val_age_output_mae: 0.1567 - val_race_output_accuracy: 0.8016 - val_gender_output_accuracy: 0.8797\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 33/64\n",
      "600/600 [==============================] - 253s 421ms/step - loss: 0.9210 - age_output_loss: 0.0096 - race_output_loss: 0.5633 - gender_output_loss: 0.3764 - age_output_mae: 0.0745 - race_output_accuracy: 0.8015 - gender_output_accuracy: 0.8047 - val_loss: 0.9692 - val_age_output_loss: 0.0067 - val_race_output_loss: 0.6120 - val_gender_output_loss: 0.2448 - val_age_output_mae: 0.0617 - val_race_output_accuracy: 0.7888 - val_gender_output_accuracy: 0.8944\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 34/64\n",
      "600/600 [==============================] - 246s 410ms/step - loss: 0.9208 - age_output_loss: 0.0096 - race_output_loss: 0.5633 - gender_output_loss: 0.3757 - age_output_mae: 0.0741 - race_output_accuracy: 0.8017 - gender_output_accuracy: 0.8019 - val_loss: 0.9633 - val_age_output_loss: 0.0085 - val_race_output_loss: 0.6025 - val_gender_output_loss: 0.2527 - val_age_output_mae: 0.0720 - val_race_output_accuracy: 0.7907 - val_gender_output_accuracy: 0.8892\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 35/64\n",
      "600/600 [==============================] - 249s 415ms/step - loss: 0.9402 - age_output_loss: 0.0094 - race_output_loss: 0.5766 - gender_output_loss: 0.3753 - age_output_mae: 0.0734 - race_output_accuracy: 0.7960 - gender_output_accuracy: 0.8054 - val_loss: 0.9165 - val_age_output_loss: 0.0073 - val_race_output_loss: 0.5744 - val_gender_output_loss: 0.2560 - val_age_output_mae: 0.0654 - val_race_output_accuracy: 0.7997 - val_gender_output_accuracy: 0.8925\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 36/64\n",
      "600/600 [==============================] - 248s 413ms/step - loss: 0.9323 - age_output_loss: 0.0093 - race_output_loss: 0.5723 - gender_output_loss: 0.3686 - age_output_mae: 0.0732 - race_output_accuracy: 0.8015 - gender_output_accuracy: 0.8068 - val_loss: 1.1340 - val_age_output_loss: 0.0561 - val_race_output_loss: 0.5883 - val_gender_output_loss: 0.2691 - val_age_output_mae: 0.2080 - val_race_output_accuracy: 0.7945 - val_gender_output_accuracy: 0.8916\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 37/64\n",
      "600/600 [==============================] - 250s 417ms/step - loss: 0.9049 - age_output_loss: 0.0091 - race_output_loss: 0.5542 - gender_output_loss: 0.3713 - age_output_mae: 0.0726 - race_output_accuracy: 0.8064 - gender_output_accuracy: 0.8007 - val_loss: 1.0050 - val_age_output_loss: 0.0199 - val_race_output_loss: 0.6015 - val_gender_output_loss: 0.2322 - val_age_output_mae: 0.1170 - val_race_output_accuracy: 0.7955 - val_gender_output_accuracy: 0.8996\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 38/64\n",
      "600/600 [==============================] - 251s 418ms/step - loss: 0.8978 - age_output_loss: 0.0092 - race_output_loss: 0.5495 - gender_output_loss: 0.3696 - age_output_mae: 0.0729 - race_output_accuracy: 0.8068 - gender_output_accuracy: 0.8081 - val_loss: 0.9799 - val_age_output_loss: 0.0171 - val_race_output_loss: 0.5872 - val_gender_output_loss: 0.3073 - val_age_output_mae: 0.1092 - val_race_output_accuracy: 0.7955 - val_gender_output_accuracy: 0.8679\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 39/64\n",
      "600/600 [==============================] - 240s 400ms/step - loss: 0.9093 - age_output_loss: 0.0096 - race_output_loss: 0.5562 - gender_output_loss: 0.3646 - age_output_mae: 0.0747 - race_output_accuracy: 0.8019 - gender_output_accuracy: 0.8032 - val_loss: 1.1408 - val_age_output_loss: 0.0551 - val_race_output_loss: 0.5971 - val_gender_output_loss: 0.2480 - val_age_output_mae: 0.2114 - val_race_output_accuracy: 0.7978 - val_gender_output_accuracy: 0.8987\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 40/64\n",
      "600/600 [==============================] - 242s 404ms/step - loss: 0.8882 - age_output_loss: 0.0091 - race_output_loss: 0.5433 - gender_output_loss: 0.3700 - age_output_mae: 0.0722 - race_output_accuracy: 0.8095 - gender_output_accuracy: 0.8071 - val_loss: 0.9482 - val_age_output_loss: 0.0065 - val_race_output_loss: 0.5972 - val_gender_output_loss: 0.2638 - val_age_output_mae: 0.0597 - val_race_output_accuracy: 0.7959 - val_gender_output_accuracy: 0.8902\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 41/64\n",
      "600/600 [==============================] - 249s 416ms/step - loss: 0.8919 - age_output_loss: 0.0088 - race_output_loss: 0.5470 - gender_output_loss: 0.3611 - age_output_mae: 0.0716 - race_output_accuracy: 0.8081 - gender_output_accuracy: 0.8111 - val_loss: 0.9752 - val_age_output_loss: 0.0191 - val_race_output_loss: 0.5803 - val_gender_output_loss: 0.2815 - val_age_output_mae: 0.1155 - val_race_output_accuracy: 0.7997 - val_gender_output_accuracy: 0.8755\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 42/64\n",
      "600/600 [==============================] - 239s 399ms/step - loss: 0.9004 - age_output_loss: 0.0090 - race_output_loss: 0.5518 - gender_output_loss: 0.3675 - age_output_mae: 0.0716 - race_output_accuracy: 0.8066 - gender_output_accuracy: 0.7983 - val_loss: 0.9539 - val_age_output_loss: 0.0065 - val_race_output_loss: 0.6019 - val_gender_output_loss: 0.2495 - val_age_output_mae: 0.0598 - val_race_output_accuracy: 0.8021 - val_gender_output_accuracy: 0.8944\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 43/64\n",
      "600/600 [==============================] - 245s 408ms/step - loss: 0.8850 - age_output_loss: 0.0092 - race_output_loss: 0.5411 - gender_output_loss: 0.3649 - age_output_mae: 0.0727 - race_output_accuracy: 0.8104 - gender_output_accuracy: 0.8024 - val_loss: 0.9589 - val_age_output_loss: 0.0075 - val_race_output_loss: 0.6028 - val_gender_output_loss: 0.2499 - val_age_output_mae: 0.0670 - val_race_output_accuracy: 0.7874 - val_gender_output_accuracy: 0.8925\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 44/64\n",
      "600/600 [==============================] - 250s 417ms/step - loss: 0.8884 - age_output_loss: 0.0088 - race_output_loss: 0.5442 - gender_output_loss: 0.3675 - age_output_mae: 0.0715 - race_output_accuracy: 0.8085 - gender_output_accuracy: 0.8079 - val_loss: 0.9963 - val_age_output_loss: 0.0213 - val_race_output_loss: 0.5910 - val_gender_output_loss: 0.2460 - val_age_output_mae: 0.1194 - val_race_output_accuracy: 0.7978 - val_gender_output_accuracy: 0.8916\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 45/64\n",
      "600/600 [==============================] - 244s 407ms/step - loss: 0.8802 - age_output_loss: 0.0086 - race_output_loss: 0.5403 - gender_output_loss: 0.3534 - age_output_mae: 0.0705 - race_output_accuracy: 0.8085 - gender_output_accuracy: 0.8130 - val_loss: 0.9713 - val_age_output_loss: 0.0108 - val_race_output_loss: 0.6006 - val_gender_output_loss: 0.2731 - val_age_output_mae: 0.0827 - val_race_output_accuracy: 0.8007 - val_gender_output_accuracy: 0.8854\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 46/64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 241s 402ms/step - loss: 0.8801 - age_output_loss: 0.0087 - race_output_loss: 0.5389 - gender_output_loss: 0.3695 - age_output_mae: 0.0706 - race_output_accuracy: 0.8108 - gender_output_accuracy: 0.8027 - val_loss: 0.9782 - val_age_output_loss: 0.0065 - val_race_output_loss: 0.6166 - val_gender_output_loss: 0.2718 - val_age_output_mae: 0.0590 - val_race_output_accuracy: 0.7841 - val_gender_output_accuracy: 0.8835\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 47/64\n",
      "600/600 [==============================] - 244s 406ms/step - loss: 0.8505 - age_output_loss: 0.0086 - race_output_loss: 0.5201 - gender_output_loss: 0.3609 - age_output_mae: 0.0704 - race_output_accuracy: 0.8150 - gender_output_accuracy: 0.8059 - val_loss: 0.9495 - val_age_output_loss: 0.0069 - val_race_output_loss: 0.5978 - val_gender_output_loss: 0.2522 - val_age_output_mae: 0.0635 - val_race_output_accuracy: 0.7983 - val_gender_output_accuracy: 0.8911\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 48/64\n",
      "600/600 [==============================] - 244s 407ms/step - loss: 0.8650 - age_output_loss: 0.0084 - race_output_loss: 0.5301 - gender_output_loss: 0.3607 - age_output_mae: 0.0695 - race_output_accuracy: 0.8135 - gender_output_accuracy: 0.8078 - val_loss: 1.0841 - val_age_output_loss: 0.0099 - val_race_output_loss: 0.6803 - val_gender_output_loss: 0.2427 - val_age_output_mae: 0.0785 - val_race_output_accuracy: 0.7694 - val_gender_output_accuracy: 0.8958\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 49/64\n",
      "600/600 [==============================] - 245s 408ms/step - loss: 0.8609 - age_output_loss: 0.0086 - race_output_loss: 0.5268 - gender_output_loss: 0.3619 - age_output_mae: 0.0705 - race_output_accuracy: 0.8179 - gender_output_accuracy: 0.8039 - val_loss: 1.0391 - val_age_output_loss: 0.0066 - val_race_output_loss: 0.6586 - val_gender_output_loss: 0.2488 - val_age_output_mae: 0.0614 - val_race_output_accuracy: 0.7822 - val_gender_output_accuracy: 0.8963\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 50/64\n",
      "600/600 [==============================] - 242s 403ms/step - loss: 0.8552 - age_output_loss: 0.0085 - race_output_loss: 0.5232 - gender_output_loss: 0.3642 - age_output_mae: 0.0699 - race_output_accuracy: 0.8145 - gender_output_accuracy: 0.8086 - val_loss: 0.9563 - val_age_output_loss: 0.0063 - val_race_output_loss: 0.6049 - val_gender_output_loss: 0.2344 - val_age_output_mae: 0.0606 - val_race_output_accuracy: 0.7983 - val_gender_output_accuracy: 0.9029\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 51/64\n",
      "600/600 [==============================] - 240s 401ms/step - loss: 0.8547 - age_output_loss: 0.0084 - race_output_loss: 0.5234 - gender_output_loss: 0.3581 - age_output_mae: 0.0699 - race_output_accuracy: 0.8133 - gender_output_accuracy: 0.8104 - val_loss: 1.0600 - val_age_output_loss: 0.0418 - val_race_output_loss: 0.5794 - val_gender_output_loss: 0.2382 - val_age_output_mae: 0.1798 - val_race_output_accuracy: 0.8011 - val_gender_output_accuracy: 0.8944\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 52/64\n",
      "600/600 [==============================] - 249s 415ms/step - loss: 0.8586 - age_output_loss: 0.0083 - race_output_loss: 0.5266 - gender_output_loss: 0.3540 - age_output_mae: 0.0695 - race_output_accuracy: 0.8169 - gender_output_accuracy: 0.8113 - val_loss: 0.9390 - val_age_output_loss: 0.0147 - val_race_output_loss: 0.5698 - val_gender_output_loss: 0.2538 - val_age_output_mae: 0.0609 - val_race_output_accuracy: 0.8021 - val_gender_output_accuracy: 0.8939\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 53/64\n",
      "600/600 [==============================] - 243s 405ms/step - loss: 0.8391 - age_output_loss: 0.0083 - race_output_loss: 0.5135 - gender_output_loss: 0.3556 - age_output_mae: 0.0693 - race_output_accuracy: 0.8182 - gender_output_accuracy: 0.8145 - val_loss: 0.9171 - val_age_output_loss: 0.0077 - val_race_output_loss: 0.5755 - val_gender_output_loss: 0.2308 - val_age_output_mae: 0.0632 - val_race_output_accuracy: 0.8059 - val_gender_output_accuracy: 0.9001\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 54/64\n",
      "600/600 [==============================] - 254s 423ms/step - loss: 0.8427 - age_output_loss: 0.0083 - race_output_loss: 0.5160 - gender_output_loss: 0.3542 - age_output_mae: 0.0694 - race_output_accuracy: 0.8157 - gender_output_accuracy: 0.8067 - val_loss: 1.1307 - val_age_output_loss: 0.0248 - val_race_output_loss: 0.6714 - val_gender_output_loss: 0.2429 - val_age_output_mae: 0.0916 - val_race_output_accuracy: 0.7827 - val_gender_output_accuracy: 0.9015\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 55/64\n",
      "600/600 [==============================] - 252s 419ms/step - loss: 0.8521 - age_output_loss: 0.0086 - race_output_loss: 0.5214 - gender_output_loss: 0.3549 - age_output_mae: 0.0704 - race_output_accuracy: 0.8137 - gender_output_accuracy: 0.8131 - val_loss: 0.9347 - val_age_output_loss: 0.0063 - val_race_output_loss: 0.5908 - val_gender_output_loss: 0.2329 - val_age_output_mae: 0.0594 - val_race_output_accuracy: 0.7945 - val_gender_output_accuracy: 0.9025\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 56/64\n",
      "600/600 [==============================] - 253s 421ms/step - loss: 0.8305 - age_output_loss: 0.0083 - race_output_loss: 0.5076 - gender_output_loss: 0.3594 - age_output_mae: 0.0694 - race_output_accuracy: 0.8184 - gender_output_accuracy: 0.8125 - val_loss: 0.9753 - val_age_output_loss: 0.0143 - val_race_output_loss: 0.5962 - val_gender_output_loss: 0.2379 - val_age_output_mae: 0.0957 - val_race_output_accuracy: 0.7983 - val_gender_output_accuracy: 0.8987\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 57/64\n",
      "600/600 [==============================] - 253s 421ms/step - loss: 0.8406 - age_output_loss: 0.0087 - race_output_loss: 0.5136 - gender_output_loss: 0.3558 - age_output_mae: 0.0706 - race_output_accuracy: 0.8148 - gender_output_accuracy: 0.8104 - val_loss: 0.9619 - val_age_output_loss: 0.0075 - val_race_output_loss: 0.6052 - val_gender_output_loss: 0.2413 - val_age_output_mae: 0.0677 - val_race_output_accuracy: 0.7988 - val_gender_output_accuracy: 0.8982\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 58/64\n",
      "600/600 [==============================] - 251s 419ms/step - loss: 0.8287 - age_output_loss: 0.0081 - race_output_loss: 0.5072 - gender_output_loss: 0.3551 - age_output_mae: 0.0686 - race_output_accuracy: 0.8197 - gender_output_accuracy: 0.8118 - val_loss: 0.9561 - val_age_output_loss: 0.0165 - val_race_output_loss: 0.5774 - val_gender_output_loss: 0.2412 - val_age_output_mae: 0.1050 - val_race_output_accuracy: 0.8068 - val_gender_output_accuracy: 0.8944\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 59/64\n",
      "600/600 [==============================] - 254s 423ms/step - loss: 0.8174 - age_output_loss: 0.0082 - race_output_loss: 0.4992 - gender_output_loss: 0.3566 - age_output_mae: 0.0689 - race_output_accuracy: 0.8195 - gender_output_accuracy: 0.8129 - val_loss: 0.9687 - val_age_output_loss: 0.0092 - val_race_output_loss: 0.6057 - val_gender_output_loss: 0.2311 - val_age_output_mae: 0.0700 - val_race_output_accuracy: 0.7955 - val_gender_output_accuracy: 0.9039\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 60/64\n",
      "600/600 [==============================] - 256s 427ms/step - loss: 0.8342 - age_output_loss: 0.0082 - race_output_loss: 0.5103 - gender_output_loss: 0.3581 - age_output_mae: 0.0690 - race_output_accuracy: 0.8171 - gender_output_accuracy: 0.8030 - val_loss: 0.9807 - val_age_output_loss: 0.0070 - val_race_output_loss: 0.6186 - val_gender_output_loss: 0.2465 - val_age_output_mae: 0.0614 - val_race_output_accuracy: 0.7907 - val_gender_output_accuracy: 0.8939\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 61/64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 256s 427ms/step - loss: 0.8148 - age_output_loss: 0.0081 - race_output_loss: 0.4975 - gender_output_loss: 0.3612 - age_output_mae: 0.0685 - race_output_accuracy: 0.8233 - gender_output_accuracy: 0.8088 - val_loss: 1.0652 - val_age_output_loss: 0.0095 - val_race_output_loss: 0.6681 - val_gender_output_loss: 0.2482 - val_age_output_mae: 0.0703 - val_race_output_accuracy: 0.7765 - val_gender_output_accuracy: 0.8963\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 62/64\n",
      "600/600 [==============================] - 255s 425ms/step - loss: 0.8260 - age_output_loss: 0.0080 - race_output_loss: 0.5058 - gender_output_loss: 0.3521 - age_output_mae: 0.0683 - race_output_accuracy: 0.8199 - gender_output_accuracy: 0.8150 - val_loss: 0.9795 - val_age_output_loss: 0.0124 - val_race_output_loss: 0.6053 - val_gender_output_loss: 0.2209 - val_age_output_mae: 0.0772 - val_race_output_accuracy: 0.7955 - val_gender_output_accuracy: 0.9010\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 63/64\n",
      "600/600 [==============================] - 255s 425ms/step - loss: 0.8224 - age_output_loss: 0.0086 - race_output_loss: 0.5018 - gender_output_loss: 0.3544 - age_output_mae: 0.0700 - race_output_accuracy: 0.8244 - gender_output_accuracy: 0.8085 - val_loss: 1.1530 - val_age_output_loss: 0.0084 - val_race_output_loss: 0.7314 - val_gender_output_loss: 0.2221 - val_age_output_mae: 0.0688 - val_race_output_accuracy: 0.7647 - val_gender_output_accuracy: 0.9062\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n",
      "Epoch 64/64\n",
      "600/600 [==============================] - 256s 427ms/step - loss: 0.8096 - age_output_loss: 0.0086 - race_output_loss: 0.4936 - gender_output_loss: 0.3463 - age_output_mae: 0.0703 - race_output_accuracy: 0.8260 - gender_output_accuracy: 0.8156 - val_loss: 0.9714 - val_age_output_loss: 0.0072 - val_race_output_loss: 0.6090 - val_gender_output_loss: 0.2901 - val_age_output_mae: 0.0611 - val_race_output_accuracy: 0.7997 - val_gender_output_accuracy: 0.8764\n",
      "INFO:tensorflow:Assets written to: .\\model_checkpoint\\assets\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "batch_size = 32\n",
    "valid_batch_size = 32\n",
    "epochs = 64\n",
    "train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size=batch_size)\n",
    "valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size=valid_batch_size)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\"./model_checkpoint\", monitor='val_loss')\n",
    "]\n",
    "history = model.fit_generator(train_gen,\n",
    "                    steps_per_epoch=len(train_idx)//batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=valid_gen,\n",
    "                    validation_steps=len(valid_idx)//valid_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1256e208",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'go' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-479d13071c2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m fig.add_trace(go.Scatter(\n\u001b[0;32m      4\u001b[0m                     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'race_output_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                     name='Train'))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'go' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['race_output_acc'],\n",
    "                    name='Train'))\n",
    "fig.add_trace(go.Scatter(\n",
    "                    y=history.history['val_race_output_acc'],\n",
    "                    name='Valid'))\n",
    "fig.update_layout(height=500, \n",
    "                  width=700,\n",
    "                  title='Accuracy for race feature',\n",
    "                  xaxis_title='Epoch',\n",
    "                  yaxis_title='Accuracy')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a009281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b56737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
